params:
  seed: ${...seed}
  algo:
    name: a2c_continuous  # a2c_discrete, sac

  model:
    name: continuous_a2c_logstd   # discrete_a2c, multi_discrete_a2c, continuous_a2c, soft_actor_critic

  network:
    name: actor_critic  # resnet_actor_critic, rnd_curiosity, soft_actor_critic,
    separate: False

    space:
      continuous:
        mu_activation: None
        sigma_activation: None
        mu_init:
          name: default
        sigma_init:
          name: const_initializer
          val: 0
        fixed_sigma: True

#try to add cnn
#在实例化Network()的时候, **kwargs传进来的:
#  input_shape=
#  actions_num=
#  value_size=1(default)
#  num_seqs=1(default)

#    cnn:
##      type:   # 'conv2d', 'coord_conv2d', 'conv1d'
##      convs:
##        filters:
##        kernel_size:
##        strides:
##        padding:
##      activation:
##      norm_func_name:   # 'layer_norm', 'batch_norm'
#      conv_depths: [32, 64, 128]   # [64, 128, 256, 512]  # [xx, xx, xx]
#    rnn:
#      units:
#      layers:
#      name:
#      layer_norm:   # default False
#      before_mlp:   # default False
#      concat_input:   # default False
#    space:
#      multi_discrete:   # default False
#      discrete:   # default False
#      continuous:   # default False
#
#    central_value:    # default False
#    joint_obs_actions:    # default None
#    separate:   # default False

    mlp:
      units: [128, 64, 32]    # [256, 128, 64]
      activation: elu
      d2rl: False   # default False

      initializer:
        name: default
      regularizer:
        name: None
      norm_only_first_layer: False   # default False

  load_checkpoint: ${if:${...checkpoint},True,False} # flag which sets whether to load the checkpoint
  load_path: ${...checkpoint} # path to the checkpoint to load

  config:
    name: ${resolve_default:YumiCube,${....experiment}}  # ${resolve_default:FrankaCabinet,${....experiment}}
    full_experiment_name: ${.name}
    env_name: rlgpu
    ppo: True
    mixed_precision: False
    normalize_input: True
    normalize_value: True
    num_actors: ${....task.env.numEnvs}
    reward_shaper:
      scale_value: 0.01
    normalize_advantage: True
    gamma: 0.99
    tau: 0.95
    learning_rate: 5e-4
    lr_schedule: adaptive
    kl_threshold: 0.008
    score_to_win: 10000
    max_epochs: ${resolve_default:3000,${....max_iterations}}
    save_best_after: 50 # 200
    save_frequency: 50 # 100
    print_stats: True
    grad_norm: 1.0
    entropy_coef: 0.0
    truncate_grads: True
    e_clip: 0.2
    horizon_length: 16  # 16
    minibatch_size: 8192  # 8192
    mini_epochs: 8
    critic_coef: 4
    clip_value: True
    seq_len: 4
    bounds_loss_coef: 0.0001
#    device: 'cuda:1'
    device: ${....ppo_device}


